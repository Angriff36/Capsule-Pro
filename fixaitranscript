 have a feeling I can guess what your stance is on AI right now. It seems pretty useful and especially helpful in
0:05
small projects, side projects, and early stage startups. But when it comes to real problems in real code bases, these
0:11
things just aren't that useful. And you don't understand why everybody's getting so hyped and excited about those things.
0:16
If that's your perspective, I understand. There's a lot of reasons most people should feel that way, and I did myself just a few weeks ago. But the
0:23
harsh reality is that things are changing, and I want to talk a bit about that. But more importantly, the mistakes
0:29
that you might be making when you use these tools that result in the lackluster experience that you're having. This stuff has changed so much
0:36
and my own understanding of how I use it has too. I keep seeing other people who are struggling and when I talk to them,
0:41
I found this set of mistakes that almost all of them seem to be making that once corrected result in really, really good
0:48
things being built using these tools. So whether you're a skeptic that doesn't see any value in these things yet, or
0:54
you're somebody who's really deep in the weeds using them every day and you just want to double check you're not making obvious mistakes, hopefully this video
1:00
will be helpful. I have a ton of sources and things I want to talk about here, and I'm really excited to do all of
1:05
that. But the first thing I want to tell you guys about is today's sponsor. It's 2026. Everything about how we code
1:11
changed last year, yet nothing about how we hire did. That's kind of insane because hiring is worse than ever now
1:17
that we're getting swarmed with terrible AI generated resumes constantly. If you're tired of reading that AI slop,
1:23
check out G2I. These guys have it figured out. They have a network of over 8,000 engineers ready to go. And these
1:28
aren't just people who are fresh out of college. These are experienced developers, many of which have worked at fang companies in the past. And they've
1:34
all been onboarded with the best modern AI tools. So, you're not going to have to catch them up the whole time that they're employed. They even offer 7-day
1:41
work trials to make sure that the employee that you get is a good fit for the company. Do you know how much easier my life would have been when I was doing
1:47
job hunting if I could have tested a company for 7 days? I honestly think both sides benefit from this. Nobody
1:53
wants to work somewhere that isn't a good fit. Nobody wants an employee that isn't either. It just makes sense to me. And if you don't think 7 days is a lot,
1:58
I have good news for you. Their process to get them to file their first PR from when you start can take as little as 7
2:05
days. After you sign up for G2I, you'll meet with their team. You'll set up a shared Slack channel, decide how often
2:10
you want to meet in chat, and within five or less days from that point, you will have your first candidates, many of
2:16
which are ready to go on the spot. And don't worry, you get to interview the candidates yourself, too. They do a first pass. They'll even take questions
2:22
that you write for them to go ask the candidates and send you video responses so you can see how the candidate
2:28
actually interacts. This is a person you're going to be employing. You want to know what they're like, not just what their credentials are on a piece of
2:34
paper. It's 2026, guys. Stop hiring like it's 95. fix it now at soyv.link/2i.
2:40
So the first thing I want to talk about, one of the biggest mistakes I see is selecting the right problem. So we start
2:46
with I have a problem. Step one, you validate the problem. Is it really a problem? Maybe this is a thing I
2:52
misunderstand or doesn't matter. But once you've decided that this is really a problem and it does matter, the next
2:58
thing you probably do is the obvious solution, if there is one. If you're looking at this problem and it's a space
3:03
you understand and the solution seems obvious, you can go try that. But if that fails, you have to do the harder
3:11
solution, which is something less obvious. Maybe you have to debug more to get more information. Maybe you have to think about it more, read more code,
3:18
check more logs, find some way to reproduce it. You have to put more time in. But once that fails, you go to step
3:25
four, which is try something new. This is the point where once your existing
3:31
solutions, your existing way of doing things has failed you, you then go try something else to see if that will solve
3:37
it. So let's say you have a database performance problem. Maybe when you do too much throughput on your database,
3:43
people start having errors in their experience. First thing to do is make sure it's really a problem. Are the errors actually happening? Are they only
3:50
happening when there is too much throughput? Are they happening for some other reason? Are they happening because the user has a weird extension or they
3:56
shipped something wrong? Once you validate the problem, you look at the error. You see what line of code is on.
4:01
And then you go make the obvious fix. You ship it. Nothing changes. You never reproduce the issue because you couldn't
4:08
figure out how to. So then you sit down, you put in the effort, you try to reproduce it. You think you find the repro for it. You ship some changes. You
4:16
put it out. Still have the same issue. At this point, you've concluded this database I'm using just cannot handle
4:22
the thing that I'm doing. So, you go explore new solutions. Maybe you look at a different database. You look at other
4:28
technologies or systems or you try something else. Or maybe you ask the AI for help at that point. And this is a
4:35
problem I see a lot. Most people don't try a new solution on a problem they already know how to solve. If you know
4:42
how to solve the problem, you just solve it. You don't use another tool to try to solve it. And this is a very common
4:49
mistake I see. People aren't using these tools to solve problems they already know and understand. They are using them
4:55
as almost like a safety net type thing once everything else they've tried has failed. And that's often the first time
5:01
they tried the thing. This is not meant to pick on Adam, but I think he's actually a really good example of this
5:07
particular problem here. Come on AI, give me that 10x power that I saw out of
5:12
Davis 7. Ben Davis is my channel manager, also a fellow YouTuber, and is having a ton of fun building with all of
5:17
these things. He wants to see how powerful this stuff can be. Here's the example he gave asking a question to
5:24
Opus 4.5 in cursor. There's a hydration error in this app. I traced it to this
5:30
file provider. TSX, can you help me find the problem? This is for a lot of
5:35
different reasons the wrong way to use AI. And thankfully, I'm already seeing people in chat realizing they're doing
5:42
the same thing. They aren't reaching for these tools to solve known problems more easily. They're reaching for them when
5:49
they've exhausted the other things within reach and then they're giving the model something that's really, really
5:54
hard or poorly understood enough to seem really, really hard. This is a really
5:59
important thing I want to push. I talked about this a little bit in the you're falling behind video and I'm going to try to not let these things overlap too
6:05
much, but one of the best things to try these tools on is problems you already know how to solve because then you can
6:13
compare your solution to the solution the tool would have done. If I know how I would fix this hydration error or this
6:19
database error and I'm curious if the AI can do it too, start there. It's a great
6:24
place to test these things out. Or another way of thinking about it, what context would you give another engineer
6:31
on your team? When I am handing problems out to earlier career engineers at my businesses, I'm not giving them the
6:38
problems that I don't understand. I'm giving them the problems that I already understand and know how to write the
6:44
code for, knowing that if I give it to that employee, it's going to take longer and maybe come out worse and maybe have
6:50
to do some back and forth with them to get it in the right shape. But I'm doing that knowingly to get it off my plate and to spread the knowledge of the
6:56
codebase around the team. Learning how to do that with AI is a skill you'll pick up over time and you'll get an
7:02
intuition not just of what problems the AI can solve, but also what things that
7:07
it needs to know it solved it right. And a fun thing you can do here is gify it a
7:13
bit and automate it a bit. If you have a problem and you know how to solve it and you give it to the agent with the same
7:19
information you have, like you saw a log that referenced a specific line of code that you're pretty sure is the problem
7:25
because of this blog post you read. Give it all of that information, all the things that led you to the solution.
7:30
Maybe just go through a PR where you fix something. Check out the commit before that fix. Hand all of what you knew to
7:37
the agent and see if it can solve that. Chances are it will. And if it doesn't, you have a thing that is as good as
7:43
gold. you have a reproducible test to see the capabilities of the model. If you have a pull request that fixes a
7:50
problem that an agent was unable to fix, go grab the code state before you push
7:55
that fix, freeze it somewhere, grab all of the information you need to solve it, put it in a markdown file in that place,
8:02
and then every time a new AI tool comes out, throw it at that problem with that frozen state and see if it can solve
8:09
that. Those types of tests are gold because benchmarks suck. Now, and if you
8:14
can collect a handful of these types of problems, the few things that the models can't do, but you could, you have a
8:21
combination of things that's useful. You have a realworld reproduction test. You
8:26
have the information needed to solve the problem and you have knowledge of what the right solution looks like and a way
8:32
to validate the solution was correct. That is a hard combination of things to get and you should be hunting for as
8:38
many of those as you can find because if you can sit on them and use them to run evals and you can tell your team or
8:44
people on Twitter or the company that made the model, hey, this update made it so these five things that the AI
8:51
couldn't do before, it can now. That is huge. And if you can sit there and say confidently, it can't solve any of these
8:57
things, it still can't solve any of these things. That is even better. That is really useful. And if you can hit me up and share some of that with me, I
9:03
will gladly pay you to have these reproductions that I can use to validate and test new things when they come out.
9:09
That is as good as gold. That said, what I think you'll discover when you do this is that when you have the right context
9:15
and the right instructions and you had enough information to solve the problem, if you give the same exact information
9:21
to the AI, it will solve the problem in roughly the exact same way. But that's just the top of it. That's just
9:27
selecting the right problem. I was touching on context management a bit there. So I'm going to go deeper on this
9:32
for the next piece. This is a project called repo mix. Its goal is to take a
9:37
given codebase and flatten all of it into a single AI friendly file so that
9:42
you could include it as contacts with chat bots. This project is a [ __ ] scourge and should be wiped off of the
9:49
internet. Not only has this project personally cost me and T3 chat in the entrance we are running probably
9:55
multiple hundreds of thousands of dollars, it also virtually guarantees you're going to get [ __ ] output. It
10:02
turns out that AI shouldn't have access to your whole codebase at once in the
10:07
context. I feel like I have to talk about this multiple times a day at this point because it happens so often, but
10:12
all AI is is autocomplete. If you hand it the capital of the US is and then
10:19
stop, it is doing a bunch of crazy math across a bunch of things called parameters and those parameters linked
10:26
some text to other text. When you have the early text of the capital of the US
10:32
is it increases the likelihood that the next pointer is pointing towards
10:37
Washington DC. The whole model that we use for LM is next token prediction.
10:43
Based on everything currently in context, all of the information passed to the model. What does it think the
10:49
next most likely 3 to six characters are? And it does that over and over again. The fact that these things can
10:56
generate real meaningful contributions to real meaningful code bases that effectively through this autocomplete
11:02
strategy is genuinely unbelievable and is an accident that it was discovered in the first place. But you have to be
11:08
conscious of this when you use the tools. If you hand a model too much
11:13
context so that the majority of the context is just nonsense about your codebase, it's just going to generate
11:20
more nonsense. Sure, there are some models that can handle way more tokens than others. Like Opus 4.5, I think is
11:27
capped at 100, maybe 200k tokens, but Gemini can do 1 to2 million. We should just all use the models with higher
11:33
context windows. No. No, we shouldn't. The more the model knows, the dumber it gets. The problem isn't that it needs to
11:40
know everything. Because the more it knows, the less likely it is to autocomplete the solution. Because if
11:46
the problem exists in the codebase, giving it the whole codebase does not give it the solution. And if you get to
11:52
the context limits, even with something like opus, once you break 50k tokens of context, the model starts to perform
11:58
worse. There is a very important concept of context rot which is when you have
12:03
too much context and it is distracting you from the thing that matters. Just to visualize it, this is the average score
12:11
that the models get for finding things in a set of repeated words. And as you
12:16
increase the number of tokens the model has, the success rates plummet fast. For
12:22
example, sonnet 4. Up to a certain point, it's 100% successful. And then once you have too much context, it goes
12:28
down to less than 60%. If the model has too much information, it will perform
12:34
worse. Stop doing this. Imagine if in every Jira ticket or linear issue you
12:39
got, the actual thing you cared about, like the problem the user had was stuck
12:44
in the middle of paragraphs upon paragraphs of [ __ ] And you have to go deep into all of it to just find the
12:52
actual problem. That's effectively what you're asking the AI to do when you compress the whole codebase in to the
12:59
history. If you copy paste your codebase into a chat product like T3 chat or
13:04
chatgbt or anything like that, it's going to confuse the hell out of it. It's not going to perform well. Don't do
13:11
that. The reason all of these new tools like cloud code, open code, and even to an extent things like cursor are doing
13:17
so well at this is because they don't give the whole codebase to the model. They give the model tools to find what
13:23
it's looking for in the codebase. Things like an agent MD or a claude MD file that briefly describes what exists where
13:30
in the codebase. Things like a GP tool to find the specific line that matters so that it can address that and only
13:37
pull in the section that matters instead of the whole codebase. Just imagine how you solve a problem. If you see some
13:43
typo on a web page, you don't go read every line of the codebase trying to find the typo. you use GP or command
13:50
shift F to find it in the codebase and then you go fix it. Models aren't that different from people. If you give it a
13:56
search tool, it will use it. If you don't and you expect it to read every line of the codebase, you're just as bad as the junior engineer that would go and
14:02
do that. People often ask, "How do you learn a codebase?" I tell them to go check the poll requests. Now, my
14:07
thoughts are a little different. But you certainly shouldn't do it by reading through it file by file. You should do it by searching around the codebase,
14:14
doing things in the app, trying to figure out where the thing you're doing exists in the code. in building the mappings and the relationships between
14:20
those things because you can't keep the whole codebase in your brain. Don't expect the model to do it. They don't work that way. So, we've established
14:27
that giving it your whole codebase is bad. But what is good context? This is a
14:33
more complex question that you'll get varying answers about. Generally
14:38
speaking, less is best. If you can simply describe the thing that is wrong
14:43
and what needs to be done, you can usually trust the model to go find what it needs to from there. This is actually
14:49
one of the things that makes codeex slightly better than Opus right now with cloud code is that Codeex will do a very
14:55
good job of reading all of the files that might be relevant, only pulling in the ones that matter, and then going and
15:02
solving the problem. it is slower as a result because it'll spend a ton of time looking into each potential file that
15:08
might be relevant, but once it does, it'll make a precise change with confidence. Whereas a model like Opus
15:14
isn't quite as eager to search the codebase and is more eager to change it. As soon as it finds something that it
15:20
thinks is the problem, it immediately goes into editing mode. So with something like Opus, it is better to put
15:26
a little more time up front to say the problem is here. Don't touch this. Don't
15:32
touch that. Just solve here. This is also an important thing to do with your claude MD file or agent MG if you're
15:39
doing other tools. I'm very thankful everyone else is following a standard and anthropic is just special snowflake.
15:45
They are what they are. They'll never embrace standards that they didn't make. It's how they are. One of the most interesting things I learned about the
15:50
Claude Code team is that whenever they notice the model doing something poorly in the Claude Code codebase, they
15:56
immediately go and change the Claude MD file to help steer the model in the
16:01
right direction. I actually just made some changes to the T3 chat codebase for the same reason. I noticed a couple
16:07
mistakes that were happening as a result of this file not steering the model in
16:13
the right direction. There are certain things that we do every day as devs that the model probably shouldn't. One of the
16:18
ones that pisses me off the most is PNPM dev. I don't want the model to run a dev
16:24
server. I have a dev server running. I want the model to make changes to know that I have a dev server running and to
16:29
just run a type check when it's done with its work. So I updated this file where I describe the PNPM scripts and
16:36
specify now note don't use this unless otherwise told to. and all of a sudden
16:42
the model randomly running dev commands stopped entirely. I also wanted it to be
16:47
able to check the types from convex because if I wasn't running a dev server or it was running in a cloud environment
16:53
where I didn't have the dev server running, it would make changes to the convex files and then not have the types update and then get really confused
16:59
about the type errors. So I added another command here, pnpm generate, and told it generate convex types after
17:05
schema changes. Now it knows when it makes a schema change, it should run this command and then the types will be
17:12
correct again. Constantly changing this file based on the mistakes you see it making seems like you're babysitting the
17:19
thing. It feels like you're doing way more work than you have to. I promise you it's not. This took 5 seconds and
17:24
it's already saved me hours. And it also helps with building the intuition for what these can and can't do. Funny
17:30
enough, when I was at Twitch, I experienced very similar things even though this was preai. When I onboarded
17:36
into the codebase for the web app when we were rebuilding it in React, I was still learning React and TypeScript. I
17:41
put up my first two PRs and had some dumb mistakes in them. Another engineer who was much more experienced hit me up
17:48
and asked me, "Hey, not trying to pick on you or anything. Why did you do this though? Why did you write this this
17:53
way?" So, I showed him the page in Twitch's docs that made me think incorrectly that I should do things a
17:59
specific way. I don't remember what it was. I just remember this order of events. I linked him the docs page that led to me doing it this way. He said,
18:06
"Oh, I see why it would make you think that. I'm going to go fix it." And before my PR even merged, they had
18:12
updated the docs to make it less likely that the next person would make the mistakes I made. The issue with AI is
18:18
that it doesn't remember things when it makes mistakes. That's your job. You have to build the memory for it. Not in
18:25
the sense that you have to manage every single parameter inside of the model, but in the sense that when you notice it making stupid mistakes the way a new
18:32
engineer might in your codebase, you need to document that so it's less likely to happen to the next person
18:38
because the AI agent is a new engineer every time it runs. The role of this file is to take this really skilled
18:44
engineer who just joined your company and make sure they know all of the things that are special about your company and your codebase. If you're
18:50
copy pasting a cloudmd file from somebody else or some template repo somewhere, you're doing it wrong. Even
18:56
if you're running the init command that a lot of these CLIs have, you're probably doing it wrong. The role of this file isn't to describe every single
19:03
thing about the codebase. It's not just docs. It is specifically a gotchas pile
19:08
almost like listing the things you've seen it do wrong to steer it away from that. This file should start really
19:15
small and simple and slowly have small additions added and tuning done to it to
19:21
steer the model away from the things you don't want it to do. And this file is always included as context. So it's
19:27
basically guaranteeing that everything in it will or won't happen depending on how you describe it. So less code, more
19:34
markdown. This is one of the most important pieces for the big code bases, too. By the way, if you're noticing
19:39
problems with agents in really big code bases, the problem isn't the size of the codebase so much as the number of
19:46
opinions and expectations that have been encoded. As a result, as the codebase gets bigger, the things that are weird
19:52
about that codebase increase, too. Your expectations around how people operate in that codebase grow. So, you need to
19:58
encode those. Another fun side effect of this is I've noticed when I look at a new codebase and I read through this
20:04
cloud MD or agents MD file, I learn a lot about how the team builds and how they think about the codebase. It's
20:10
actually a really useful way to learn. Honestly, context management is complex enough that I could do a whole series of
20:16
videos on it. It's not complex in the sense that it's like super super hard to figure out. It's complex in the sense
20:23
that there are many different solutions to the weird set of problems and you can for the most part ignore them and be
20:29
fine. So don't overthink this one. Don't try to hack this one either. It's a common mistake. That's kind of that
20:35
stupid repo mix thing I was sharing earlier. It's kind of the problem with things like this. It's trying to hack
20:40
the context instead of use the context. And hacking the context doesn't work.
20:46
Don't try to work around this reality. Try to embrace it. recognize the fact that these problems shouldn't require
20:53
knowing everything about the codebase. Again, to go back to the selecting the right problem thing, if you had a really
20:58
smart friend that you were giving access to this codebase, what information should they have in order to solve the
21:04
problem that you're describing? Assuming they know nothing about the codebase, what is the smallest amount of info you
21:09
can give them for them to meaningfully understand and solve the problem? That is what you should give to the agent.
21:15
We'll probably talk a little more about context management in other pieces here and I might even do more videos about it
21:20
in general in the future. But you have enough of an idea now about the common mistakes I see. Hopefully that'll help
21:25
out enough that you can get more value from these tools. Speaking of which, one of the biggest problems I see using
21:32
outdated tools or even worse holding your perspective based on experiences
21:37
you had with outdated tools. Let's be real. If your perspective on how good AI
21:42
is at coding is based on your experience using sonnet 3.5 in wind surf in the
21:48
early days of agentic coding where you handed it the whole codebases context and didn't have a cloudmd or an agent md
21:54
file to help it out. You're in a different world than us. That's legitimately like comparing notepad.exe
22:01
to vim. Like the gap here is insane. To be fair to the people who feel this way,
22:08
this is not just a cope. This is a learned behavior from a history of doing
22:13
stuff with software and in the development world. If you tried adopting GraphQL right when it started getting
22:19
hype and it went really poorly for you and then you tried again four years later, it didn't get much better. And
22:26
that's the case a lot of the time. There are things that have a lot of hype that you go try, they just don't work well
22:32
for you. then going back a few years later is very unlikely to change anything. That has been the case in the
22:38
dev world for a while. If you tried React a couple months after it dropped and you just hated the syntax, the way
22:43
it worked and no matter how hard you tried, you couldn't get over it, coming back a year later isn't going to be any
22:48
better. Cuz like as much as React has changed, it hasn't changed in ways that would turn a skeptic into a supporter.
22:54
It just improved in ways that take supporters and make them more powerful. So, if you used these tools two years
22:59
ago and got nothing out of it and you've based your way of evaluating things on how you've evaluated them your whole
23:05
lives, chances are this thing isn't going to be any better. I understand why you're avoiding it. Things are changing
23:12
absurdly fast. I had problems that nothing in the AI world could solve 6
23:17
months ago that started being solved three months ago. Like, they went from, "Oh, haha, AI can never solve this," to,
23:24
"Wait, what the fuck?" in literally a 3-month window. That is insane. This level of oh [ __ ] has not happened in the
23:30
software dev world almost ever before where a thing could go from incapable of something to beyond capable at the exact
23:38
same thing in months. So if you're not using the state-of-the-art stuff, you don't know where it is today. And I know
23:44
a lot of you guys aren't using it because I've seen the numbers. Not the numbers of how much you're using certain
23:50
models or how many tools you're using and when you use them. I'm just talking about my sub count. Did you know less
23:55
than half of you guys are subscribed to my channel? How are you expecting to keep up on all of this [ __ ] I can barely do it and it's my job to. If you
24:02
want good summaries of what's going on and what the hot [ __ ] is and what you can learn from it and whether or not to try it, hit that red button. It costs
24:08
you nothing and it helps you a ton with keeping up. I guarantee you that that engineer on your team that is way ahead
24:14
that is killing it with all these AI tools, they're probably subscribed and you're not. Fix it. Anyways, I could sit
24:20
here and tell you what the hottest tools are right now, but I want this video to stay useful for a while, so I'm not
24:27
going to do that. You can go check Twitter or you can go to my channel and sort by most recent and get a rough idea
24:33
of what is good enough today. I will say chances are it's probably not co-pilot.
24:40
And also important thing, if your company doesn't let you try the new cool
24:45
hot things, this is harder to do. And I actually do empathize with that. If you applied to get co-pilot approved at the
24:51
start of 2024 and it finally got approved halfway through 2025 and
24:57
everybody's moved over to Cursor and Cloud Code and Codeex, you now have to wait a year and a half for it to be approved again. That sucks. There are
25:04
reasons that you're on outdated tools that aren't your fault. And if that's the case, first off, I'm sorry. Second
25:09
off, you should start ignoring those rules and doing it anyways. If you get fired, you have an awesome story. Ask forgiveness, not permission. And third,
25:15
you can probably find a better company to do these things at because this stuff like like don't fall behind because your
25:20
company doesn't let you try new things. And you'd also be amazed how many big companies are embracing this. There are
25:26
medium-sized companies like Sentry that are big enough to buy Super Bowl ads that will let you bring in whatever
25:32
tools you want. And then there are giant companies like Microsoft that will also do the same. I can't tell you how many
25:37
of the startups I've invested in that have Microsoft as a customer because they're actually willing to try out these new tools. I think that's really
25:43
cool, especially because they compete with a handful of them. But then there's places like Amazon where they are constantly being told they can't use
25:49
these tools because they really want to make their crappy vibe coding, PRD building. I think it's called Kira is
25:56
their VS Code fork. There are a lot of people at Amazon that are forced to use that and can't use better tools because
26:02
Amazon really wants the information in order to make their tool better. So that sucks. And if you're in one of those boats, figure it out. I'm sorry. It
26:09
sucks. I get it. But the fact that your company is forcing you to use worse tools does not mean the tools are
26:15
useless. The ones you're using might be useless. And I know you guys don't know many people using Kira, but all the Kira
26:21
users I know are my ex-coorkers that are still on Amazon and Twitch cuz they're all forced to use it. Yeah. So if
26:26
they're skeptical of AI tools, I get it because Yeah. But you should find
26:32
opportunities to try the best things. I am regularly amazed. Like when I started using Cloud Code with Opus, after using
26:38
cursor with Opus, my brain just kept opening to the things you could do with it. Next common mistake, and I really
26:45
hope people aren't making this, but honestly, I I know you are because I've seen it a whole bunch and I've made this
26:51
mistake myself too in a handful of places. Broken environments. This one is so common. Good example here. I've been
26:58
in code bases that are mono repos where if you open the repo at a root level in your editor, the type system breaks. you
27:05
have to open the sub packages in order for the TS config to work. That's a broken environment. If I can't in the
27:12
root of the project run a type check command and know that everything in the project is handled properly, your
27:18
environment is broken. And this is a very common thing. I've seen this so many times. If your type check involves
27:25
a CD, an actual directory change, your environment is broken. And you can go
27:30
try to teach the agent about all of these things through the MD files through agent MD or cloud MD, but it's
27:36
much better to just fix your [ __ ] environment. I just had this in a Vibe coding project. I was working in the Vibe canban codebase and they had a
27:43
mistake in their ESLint config that meant that whenever you opened a file in the editor and you were in the root, not
27:49
the subpackage, every single file wouldn't be type checked and threw an error on the first line because it
27:56
expected you to be using it at the root. fix those things. Not just for the agents, but for your co-workers, too. If
28:02
I open up your project in my codebase and I go to a file and it's getting a type error from main because your
28:09
configuration is bad, fix it. A good rule of thumb here is if you have a good
28:14
engineer that has never touched your codebase and you ask them to spin up on it and they run into these types of
28:20
things, you shouldn't tell them why they're wrong or how to fix it. You should go fix the codebase so these don't happen at all. Especially because
28:26
again these agents effectively have their memory wiped every time you run them again. So if you have this type of
28:32
type error where every file has a bad eslint config every single time the
28:37
agent runs it's going to see the error. It's going to try and fix the error. It's going to realize the error isn't because of their changes. And I can tell
28:43
you how many times we've seen this. The agent finds the file that it needs to touch to fix the problem. It makes the
28:49
changes. It fixes the problem. It verifies the fix. It runs the type check. It gets a type error. It freaks
28:55
the [ __ ] out. It tries a billion random things to fix it. Eventually, it reverts
29:00
the change. It sees the same error. It then says to itself, "Oh, I guess this error already existed and is unrelated
29:07
to my changes. It reapplies the change and then it finally ships." And then you run the agent with a different problem
29:12
and the exact same thing happens over and over and [ __ ] over. If you tell
29:17
an AI agent about a ghost, it will chase it forever. You need to get rid of the ghosts. You need to get those skeletons
29:24
out of the closet. And do you know what's really crazy? If you see these issues, you can tell the agent about
29:30
them and it'll fix them really well. Do you know how I fixed the problem in the Vibe canband codebase? So my issue is
29:35
when I went in a file, I had a type error. So what I did to solve this problem is I didn't feel like dealing with the TS config [ __ ] I literally
29:42
clicked in cursor the little button that appears that says oneclick fix this
29:47
error. Ask the agent to fix the error. I clicked that. It put this in the text here. For this code present, we get the
29:54
following error. Parsing error. Cannot read the file tsconfig.json. Fix it. Verify. And then give a concise
30:00
explanation. And it did it. It realized the problem is that the project expected the tsconfig to be at dot slash. So
30:08
where we currently are, but it's a monor repo, so it doesn't actually exist there if you're using it at the top level. If
30:13
I had opened my editor in the front-end folder, it would have been fine. But I opened my editor at the root, so it
30:19
wasn't. So what we need to do is use the path helper to figure out where we are
30:25
so we can actually link it correctly. And it did this with no issues in one shot. I then pushed it up, got it
30:32
merged, and now the agents will behave better when they work in this codebase. So the very problem you are giving
30:37
agents can actually be solved by those very same agents. You need to fix your environments. I I have played in far too
30:45
many big code bases that have lots of problems like this. You can fix them yourself. You can fix them with an
30:50
agent, but please [ __ ] fix them. The next time I see somebody who can't get AI to solve their problem because their
30:56
environment is broken, I'm going to flip a [ __ ] Obviously, if you have type errors everywhere because you suck at
31:01
setting up the environment, things are going to break. Fix it. Okay. The next mistake I see is actually one I'm scared
31:08
about because I have a feeling this video might contribute to it. I almost want to put this earlier, but I'm going
31:14
to put it here because this is where it starts to matter. MCP Helen over configuration. I cannot tell you how
31:19
many times I have seen this. People spend so much time trying to set the agents up to succeed that they lose the
31:27
plot. They bloat the context with dozens of MCP servers and then nothing works.
31:33
As you know, I've been using Cloud Code a lot. Want to see all the MCP servers I have configured? Oh, literally none.
31:40
Zero. Same deal with cursor. Same deal with every other tool I use. Stop
31:45
loading up your agents with these useless [ __ ] things. Just don't. It's so common, too. I've seen this a ton.
31:53
Chad immediately says, "Now, let's see your skill section." Okay, I have one skill. The front-end design skill. Do
32:00
you know what the front-end design skill does? It is a single markdown file that tells the model to not make AI slop. It
32:07
literally says exactly that. This skill guides creation of distinctive production grade front-end interfaces
32:13
that avoid generic AI slop aesthetics. Implement real working code with exceptional attention to aesthetic
32:18
detail and creative choices. Never use generic AI aesthetics like overused font families, clich√©ed color schemes,
32:25
particularly purple gradients on white backgrounds. This is all that skill is.
32:30
That's all I have for configuration in cloud code. I have a custom runner CC
32:36
that is a oneline addition to my zish file that adds a specific environment
32:42
variable in front to hide my email address and also appends the d-dangerously
32:48
skip permissions. I am tempted to put a piece of advice that is always use mode
32:54
and dangerously skip permissions, but you'll get there yourself. I don't need to tell you this. Stop adding all of
32:59
this [ __ ] to your stuff, though. skills are literally just markdown files and they're often way longer than they
33:04
should be. They are for the most part context bloat and context rot. Avoid them unless they solve very specific
33:10
problems that you are specifically having after you've tried solving them other ways. And I've talked enough about why MCP servers suck. You should already
33:17
understand that by now. If not, go watch my other videos. If you are skeptical of AI being useful in your day-to-day work
33:23
and you are touching MCP at all, you are [ __ ] up. If you don't see the value in these things yet, don't configure
33:29
[ __ ] Don't go into MCP. Don't add a whole bunch of skills. Don't add hundreds of rules to cursor. Make subtle
33:35
adjustments to your cloud MD and your agent MD file. And try to give a bit more clarity in your prompt as you talk
33:42
to the model. One more thing, and I feel bad calling this out cuz it seems like a pretty good and very well-intentioned
33:47
project. Oh, my open code. This seems fine if you're already deep in open
33:53
code, getting a lot of value from it, and you want to see what it looks like to thoroughly configure it. That's the
34:00
point of oh my open code. But we're already seeing in chat, oh my open code is so bloated, slop. Yeah,
34:10
it's a mixed bag at best. You don't solve problems with AI coding tools by
34:15
adding more things to them. More features, more MCPs, more plugins, more skills, none of that's going to make you
34:22
go from this thing is useless to this thing is useful. If anything, they're going to make it feel worse. I promise
34:27
you if you take somebody who isn't sure about these tools and you clone them, you give one version of this person
34:34
stock cla or codecs and tell them to solve problems, you give the other one a super customized oh my open code setup
34:41
with all the fancy MCPS, all of the skills built in and everything, the person with stock codec is going to have
34:46
a much better time and going to enjoy it a lot more. These things can be useful in small doses once you have specific
34:52
problems that they solve, but nobody talking about them is there. Everybody talking about this [ __ ] is just AI
34:58
maximalist trying to squeeze every single possible thing. It's the same people who would buy Android phones
35:04
because they cared so much about every single line in the specs. I'm saying this cuz I was one of those people. I
35:09
used to tell you very excitedly how many gigahertz were in my phone. I was so hyped when I got my first dual core
35:15
phone back in the day. I don't even know how many [ __ ] cores my iPhone has. I have literally no idea. I know it's
35:21
fast. I know the GPU is even faster, but I have no [ __ ] idea what the specs of this phone are. I have better [ __ ] to
35:26
do, and you do, too. Stop adding all these [ __ ] tools. The tool maximalist people are the same ones who care too
35:32
much about specs when your grandma's buying a computer. It's the exact same people. Ignore them and go get real work
35:38
done. I feel like Pete comes up in almost all of my videos at this point, but there's a reason why. He is building
35:44
an absurd amount of things in parallel, all for free and open source, because he's already made his money, and he's
35:50
killing it. He's doing really, really cool [ __ ] And he's vibe coding almost all of it. And he's vibe coding so hard
35:56
that he's putting out 500 plus commits a day regularly. So he must be maximizing
36:01
all of this stuff, right? All the customization, all the MCPs, all the skills, all the things. He's probably
36:07
running his own forked version of whatever. No. Ready to see what he uses?
36:12
He uses stock codecs. Here is his config for codecs. Defaults to GBD 5.2 Codeex
36:19
high. defaults to high reasoning, has a limit for how many tokens it can output, has a model autocompaction limit, and
36:25
then in the features, ghost commits are off, unified exec is true, free form patch is true. It's just a tool they
36:32
have for doing patching better. I think it's on by default now. Web search request is true because for whatever
36:37
[ __ ] reason, the search tool for searching the web is off by default in codeex. This is like the only part in
36:43
here that really matters. Skills is true cuz he's playing with skills. And then the shell snapshot is true. I don't even know what that does. This allows the
36:49
model to read more in one go. The defaults are a bit small and can limit what it sees. It fails silently, which is painful and something they'll
36:55
eventually fix. Also, web search is still not on by default. Yeah, the unified exec apparently replaces using
37:01
T-mucks for handling execution. Not much, but like that's it. No crazy plugins, no crazy forks, no absurd
37:08
orchestration stuff. Keep it simple. If the thing isn't useful in the simple form, it's not going to magically get
37:14
useful by adding a bunch of [ __ ] Please stop. I actually really like this article he did called just talk to it.
37:21
This is as people realize over time what they can do with these things. They start with please fix this and as they
37:28
get more in they do crazy things. Eight agents running at the same time, complex orchestration with multiple checkouts,
37:34
chaining agents together, custom sub agent workflows, libraries with tons of slash commands, full stack features
37:40
being built in MCPs, all that [ __ ] And once you get through all that, you realize the best thing to do is just say, "Hey, look at these files." and
37:46
then do this thing. Life is much better when you realize that's all you need. That said, even he admits he'll
37:52
sometimes just take a screenshot of a thing that's wrong, paste it to the model, and say, "Fix this," and it will.
37:58
Last important piece here, plan mode and iteration. A very common mistake I will
38:03
see is someone asks the agent to do something, usually compounded with the other mistakes, like with a bunch of
38:09
weird tools on, nowhere near enough context on the problem, and then it struggles. And rather than fix the
38:15
context, they keep asking more things and adding on. They keep appending more
38:21
stuff as the mistakes compound. And remember, this is all autocomplete. So if the history that it has is a bad
38:28
instruction, incorrect implementation, and then a correct instruction, there is
38:33
still more bad information than good. If the whole thing is autocomplete based on the history, having a bunch of bad
38:40
things in the history is going to result in bad output. even put a good thing at the end. Obviously, it can work through
38:46
this and there are plenty of examples of it doing it. But it is significantly better to not do this this way. When you
38:53
notice that the output came out bad rather than try to fix it with a better input being appended, revert, go back,
39:01
make the better input the start. Because if you have a better input as the start, the likelihood that the output is better
39:07
too is much much higher. The more good context exists in your history, the more
39:14
likely the next thing is good as well. Obviously, there's a limit here. If you have too much context, chances are the
39:20
context has become less relevant, which makes it worse, which makes the output worse, too. Fix it. And you know what?
39:25
One of the best ways to fix this is plan mode. Plan mode is great because instead
39:31
of the bad input resulting in a bad output, the output will be a confused
39:36
output. In fact, it'll be confused questions where instead of it doing a whole bunch of things it shouldn't, it
39:42
will add a little bit of additional context. That is, hey, I'm not sure about these three or four things. Can
39:49
you answer these questions for me? And you might realize, oh, I should have put that at the start. And you can kill it
39:54
and restart if you want. Or you can answer the questions. Those answers to those questions result in better
40:02
answers. And then the model can output a good plan. And now the majority of your context and
40:09
the majority of your history is useful. So the likelihood that it builds something good is much higher. This is
40:16
why plan mode is good. It gives the right relevant context instead of handing the model your whole codebase or
40:22
a bunch of tools to find things in the codebase. The plan mode creates a perfect prompt almost. The goal of plan
40:29
mode is to write the exact thing you want to hand to the model that it can then go use to solve the problem. But a
40:36
really important piece here is what happens if it goes wrong. If you make this plan, you think it's good, you run
40:42
it with the model and it comes out with a bad output. If it's just a few things that are wrong, sure, tell it that. Tell
40:49
it to go fix those small things and it will. But if it gets the whole thing wrong, reflect on why. Read a little bit
40:56
of what it did. Read the reasoning traces for why it made the change you don't like or the thing that you don't
41:01
want. If it's something that was wrong with the plan, go fix the plan. If it's something that was wrong with the
41:07
understanding of the code base, go fix your claw MD or your agent MD file. This is the delicate balancing act you have
41:13
to do. And it's an intuition you build as you do it more. You'll write a plan, the model will start executing on it.
41:18
You catch something it's doing wrong and you'll just know where to go to fix it. Just to go back to my example earlier in the T3 recheck codebase when I ran a
41:26
plan and it got the code right, but then it ran a dev server and it couldn't verify things because the dev server
41:31
went to the wrong port, caused errors, and broke [ __ ] I knew the solution wasn't to add to the plan. Don't run dev
41:37
commands. I know that cuz I used to do that. Instead, I went and put it in the agent MD file and then cloned it as the
41:42
cloud MD file and the problem disappeared. Figuring out where to put the thing to stop the bad behavior is an
41:49
intuition you'll build and you'll build it way quicker than you think. The same way you build intuitions about how to use a tool, right? Like in React, you
41:56
don't put hooks after an early return. You just get why after you use it for a bit. Those types of things are
42:02
intuitions you build as you use the tools, but you have to build them, not just keep saying fix it over and over
42:07
and over again. Generally speaking, if you're not oneshotting things often, that's because there are problems in
42:14
your prompting, problems in your context management, problems in your agent MD and cloudmd files, problems in the
42:19
environment you have given the model to solve things in. And as you catch mistakes on the other side, there are
42:25
more solutions there, too. If you notice the model makes code that works, but the types are always erroring, go give it a
42:30
command to check the types so that it knows they are good. A lot of things now have type-checking plugins built in with
42:36
the LSPs. I've had mixed success with those personally, but giving it a type-checking command can solve almost
42:42
all of these problems. Don't overengineer it. Just identify the common mistakes and fix those in the
42:48
agent MD or in a given plan. If a mistake happens, don't tell it to correct it. Go back and adjust the plan
42:54
and rerun it. So, to wrap things up, I want to go back to this tweet from Adam in the back and forth we had on it. He
43:01
couldn't get it to solve this error. He had done a little bit of work. He found the file that it is likely coming from.
43:07
He knew it was from this provider tsx file, but he couldn't figure out why. He also wasn't very clear in the ask. He
43:14
had an ask mode and ask it if it could help him find the problem. It's not a big deal. Models can work through that
43:20
type of thing, but being more concise and specific about what you want from it helps a lot. The bigger problem here
43:26
though is that he didn't know what the problem was and hadn't found the right context for it. The feedback I gave was
43:32
that he should copy paste the exact error. give it any useful context like does it happen to logged in or logged
43:37
out users other information that could make it easier to root cause the bug also with a case like this because it's
43:43
happening in the browser a tool that I can use to verify its fixes like playright or access to the browser
43:48
directly can be really helpful I also specified put in less qualifiers instead of can you help me find it it's what is
43:54
the cause of this error and if you think the model can fix it just skip straight to build mode turns out the reason that
44:00
Adam couldn't do this is because he didn't know that there was an exact error. His immediate response was there
44:05
is no exact error. Is there isn't the only framework that manages to actually give you a diff when the HTML is broken
44:11
with a hydration error? And this is again to go back to the start why he was using the model. He was using a model to
44:17
solve this problem because he had tried all of the other things he knew of and they didn't work. So he was using the
44:22
model for the first time as a last fallback and that's not going to work
44:27
great. If you're only using these models because you don't have enough information to solve the problem, you're not using them right. And what made this
44:33
one really funny is that I effectively played the role of what the you wanted the model to do here. I told him that
44:40
this was added in React 19 that you have traces that now show you in React where
44:45
the hydration error occurs. Ah, I'm an idiot. The diff was right in front of my
44:50
face. What's crazy though is that it appeared to understand perfectly what a hydration error is, but it diagnosed the
44:57
cause wrongly, even absurdly. Almost exactly backwards. Yeah, because it
45:02
didn't have enough context because you as well didn't have enough context. And once he found the right error and handed
45:09
that to the model, sure as [ __ ] it could solve it perfectly. So again, if you apply the lessons from what I just
45:17
explained here, you have to select the problem well. You have to not select a problem that you don't know how to solve. Ideally, you have to give the
45:24
right context and find the context if it doesn't exist. Help the model work on it with you if it doesn't and you want to
45:30
build it. He wasn't using outdated tools. All that stuff was fine. The environment was kind of broken in the sense that there was no way for it to
45:36
know the right way. But if he was to go add everything, he would quickly end up in MCP hell. So finding the right balance here, a hard thing to do, but if
45:43
you do it right, it's great. But with the right application of a handful of these tips, he would have had a much better time. And after that back and
45:49
forth, he is now enjoying these tools much more. I hope I've done an okay job of explaining why I like these things
45:54
and the mistakes that I've seen others having with them. And maybe, just maybe, you'll be a bit more of a vibe coder as
46:00
a result. At the very least, you have a whole new set of things to flame me for in the comments. Let me know how you feel about this one. And until next
46:06
time, peace nerds.